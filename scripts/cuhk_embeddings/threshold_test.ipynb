{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7e2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch3/atharva/anaconda3/envs/rescueCLIP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging.config\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from rescueclip.logging_config import LOGGING_CONFIG\n",
    "\n",
    "logging.config.dictConfig(LOGGING_CONFIG)\n",
    "logger = logging.getLogger(__name__)\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from typing import cast, Sequence, List, Any, Literal\n",
    "import weaviate\n",
    "from tqdm import tqdm\n",
    "from weaviate.classes.query import Filter, MetadataQuery\n",
    "from weaviate.collections.classes.types import Properties, WeaviateProperties\n",
    "from weaviate.collections.classes.internal import Object\n",
    "from weaviate.util import generate_uuid5, get_vector\n",
    "\n",
    "from rescueclip import cuhk\n",
    "from rescueclip.cuhk import SetNumToImagesMap\n",
    "from rescueclip.ml_model import (\n",
    "    CollectionConfig,\n",
    "    CUHK_Apple_Collection,\n",
    "    CUHK_Google_Siglip_Base_Patch16_224_Collection,\n",
    "    CUHK_Google_Siglip_SO400M_Patch14_384_Collection,\n",
    "    CUHK_laion_CLIP_ViT_bigG_14_laion2B_39B_b160k_Collection,\n",
    "    CUHK_MetaCLIP_ViT_bigG_14_quickgelu_224_Collection,\n",
    "    CUHK_ViT_B_32_Collection,\n",
    ")\n",
    "from rescueclip.weaviate import WeaviateClientEnsureReady\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from embed_cuhk import Metadata, embed_cuhk_dataset\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c785fec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-22 13:12:53,635 [INFO] rescueclip.weaviate: Weaviate is ready\n"
     ]
    }
   ],
   "source": [
    "collection_config = CUHK_Apple_Collection\n",
    "client = WeaviateClientEnsureReady().get_client()\n",
    "collection = client.collections.get(collection_config.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c5a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-22 13:12:53,672 [INFO] __main__: Number of objects 18596\n"
     ]
    }
   ],
   "source": [
    "QUERY_MAXIMUM_RESULTS = 200_000\n",
    "\n",
    "number_of_objects: int = collection.aggregate.over_all(total_count=True).total_count # type: ignore\n",
    "logger.info(f\"Number of objects %s\", number_of_objects)\n",
    "assert (\n",
    "    number_of_objects <= QUERY_MAXIMUM_RESULTS \n",
    "), \"Ensure docker-compose.yml has QUERY_MAXIMUM_RESULTS to greater than 200_000 or the experiment's results may be inaccurate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9694d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-22 13:13:29,823 [INFO] rescueclip.cuhk: After filtering, using 4649 sets and 18596 images\n",
      "2025-02-22 13:13:29,826 [INFO] __main__: Total series: 4649\n",
      "2025-02-22 13:13:29,826 [INFO] __main__: In-sample series: 2324\n",
      "2025-02-22 13:13:29,827 [INFO] __main__: Held-out series: 2325\n",
      "2025-02-22 13:13:29,827 [INFO] __main__: Retrieving the entire database into memory\n",
      "2025-02-22 13:13:33,202 [INFO] __main__: X.shape: (18596, 1024)\n",
      "2025-02-22 13:13:33,205 [INFO] __main__: y_set_labels.shape: (18596,)\n",
      "2025-02-22 13:13:33,206 [INFO] __main__: X_train.shape: (9296, 1024)\n",
      "2025-02-22 13:13:33,206 [INFO] __main__: y_train_set_labels.shape: (9296,)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "# Remove one random image from each series\n",
    "INPUT_FOLDER = Path(os.environ[\"CUHK_PEDES_DATASET\"]) / \"out\"\n",
    "STOPS_FILE = Path(\"/scratch3/gbiss/images/CUHK-PEDES-OFFICIAL/caption_all.json\")\n",
    "sets = cuhk.get_sets_new(INPUT_FOLDER, STOPS_FILE)\n",
    "sets = cuhk.keep_sets_containing_n_images(sets, 4)\n",
    "\n",
    "set_number_set_list_pairs = list(sets.items())\n",
    "np.random.shuffle(set_number_set_list_pairs)\n",
    "\n",
    "_in_sample_series = set_number_set_list_pairs[: len(set_number_set_list_pairs) // 2]\n",
    "in_sample_series = {set_num: file_names for set_num, file_names in _in_sample_series}\n",
    "_heldout_series = set_number_set_list_pairs[len(set_number_set_list_pairs) // 2 :]\n",
    "heldout_series = {set_num: file_names for set_num, file_names in _heldout_series}\n",
    "\n",
    "logger.info(f\"Total series: {len(set_number_set_list_pairs)}\")\n",
    "logger.info(f\"In-sample series: {len(in_sample_series)}\")\n",
    "logger.info(f\"Held-out series: {len(heldout_series)}\")\n",
    "\n",
    "\n",
    "logger.info(\"Retrieving the entire database into memory\")\n",
    "results = collection.query.fetch_objects(\n",
    "    limit=QUERY_MAXIMUM_RESULTS,\n",
    "    include_vector=True,\n",
    "    return_properties=True,\n",
    ")\n",
    "assert len(results.objects) == number_of_objects, \"Expected the entire database to be retrieved\"\n",
    "X = np.array([obj.vector[\"embedding\"] for obj in results.objects])\n",
    "y_set_labels = np.array([obj.properties[\"set_number\"] for obj in results.objects])\n",
    "X_indices_in_insample_series = np.array(\n",
    "    [\n",
    "        i\n",
    "        for i, image_metadata in enumerate(results.objects)\n",
    "        if image_metadata.properties[\"set_number\"] not in heldout_series\n",
    "    ]\n",
    ")\n",
    "X_train = X[X_indices_in_insample_series]\n",
    "y_train_set_labels = y_set_labels[X_indices_in_insample_series]\n",
    "\n",
    "logger.info(f\"X.shape: {X.shape}\")\n",
    "logger.info(f\"y_set_labels.shape: {y_set_labels.shape}\")\n",
    "logger.info(f\"X_train.shape: {X_train.shape}\")\n",
    "logger.info(f\"y_train_set_labels.shape: {y_train_set_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932df311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_out_neighbors_in_heldout_set(neighbor_objects: Sequence[Object[WeaviateProperties, Any]], heldout_series: SetNumToImagesMap) -> Sequence[Object[WeaviateProperties, None]]:\n",
    "#     result = []\n",
    "\n",
    "#     for objectt in neighbor_objects:\n",
    "#         set_num = objectt.properties[\"set_number\"]\n",
    "#         file_name = objectt.properties[\"file_name\"]\n",
    "#         if set_num in heldout_series:\n",
    "#             if file_name in heldout_series[set_num]:\n",
    "#                 result.append(objectt)\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f1e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=0.1 | In-sample: True: 100%|██████████| 2324/2324 [02:47<00:00, 13.91it/s]\n",
      "t=0.1 | In-sample: False: 100%|██████████| 2325/2325 [02:46<00:00, 13.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "@dataclass\n",
    "class ConfusionMatrix:\n",
    "    tp: int = 0\n",
    "    tn: int = 0\n",
    "    fp: int = 0\n",
    "    fn: int = 0\n",
    "\n",
    "    def update(self, *, tp: int = 0, tn: int = 0, fp: int = 0, fn: int = 0):\n",
    "        self.tp += tp\n",
    "        self.tn += tn\n",
    "        self.fp += fp\n",
    "        self.fn += fn\n",
    "\n",
    "    def as_array(self):\n",
    "        # Returns a 2x2 array: [[tn, fp], [fn, tp]]\n",
    "        return [[self.tn, self.fp], [self.fn, self.tp]]\n",
    "    \n",
    "    def precision(self):\n",
    "        if self.tp + self.fp == 0:\n",
    "            return 0.0\n",
    "        return self.tp / (self.tp + self.fp)\n",
    "\n",
    "    def recall(self):\n",
    "        if self.tp + self.fn == 0:\n",
    "            return 0.0\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "\n",
    "    def f1(self):\n",
    "        prec = self.precision()\n",
    "        rec = self.recall()\n",
    "        if prec + rec == 0:\n",
    "            return 0.0\n",
    "        return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"TP: {self.tp}, TN: {self.tn}, FP: {self.fp}, FN: {self.fn}\"\n",
    "\n",
    "def get_set_number_of_neighbors_within_t_ordered_by_t(vector: np.ndarray, t: float, X: np.ndarray, y_labels: np.ndarray, distance_metric: Literal['cosine']) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return the set numbers of the closest neighboring images within t, ordered by t.\n",
    "    \"\"\"\n",
    "    assert X.shape[0] == len(y_labels), \"Expected X.shape[0] == len(y_labels)\"\n",
    "    assert len(vector) == X.shape[1], \"Expected len(vector) == X.shape[1]\"\n",
    "\n",
    "    if distance_metric == 'cosine':\n",
    "        # Get the distances between the test image and all the images in X\n",
    "        distances = cdist(X, [vector], metric='cosine')\n",
    "        assert distances.shape == (X.shape[0], 1), f\"Expected distances.shape == (X.shape[0], 1), got {distances.shape}\"\n",
    "        distances = distances.reshape(-1)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"distance_metric {distance_metric} not implemented\")\n",
    "    \n",
    "    assert distances.shape == (X.shape[0], ), f\"Expected distances.shape == (X.shape[0], ), expected {(X.shape[0], )}, got {distances.shape}\"\n",
    "\n",
    "    arr = distances\n",
    "\n",
    "    # Get the indices of the closest images within t\n",
    "    indices = np.where(distances <= t)[0]\n",
    "    indices = indices[np.argsort(distances[indices])]\n",
    "\n",
    "    sorted_distances = distances[indices]\n",
    "    sorted_y_lables = y_labels[indices]\n",
    "    \n",
    "    return sorted_y_lables\n",
    "\n",
    "def threshold_test(sample_series, X_train, y_train_set_labels, t, confusion_matrix: ConfusionMatrix, is_in_sample=True) -> None:\n",
    "    for sample_serie in tqdm(sample_series.items(), total=len(sample_series), desc=f\"t={t} | In-sample: {is_in_sample}\"):\n",
    "        set_num, images_fps = sample_serie\n",
    "        for image_fp in images_fps:\n",
    "            # Get the object for image_fp\n",
    "            objects = collection.query.fetch_objects(\n",
    "                filters=Filter.by_property(\"set_number\").equal(set_num)\n",
    "                & Filter.by_property(\"file_name\").equal(image_fp),\n",
    "                include_vector=True\n",
    "            ).objects\n",
    "            assert len(objects) == 1, \"Expected 1 object\"\n",
    "            objectt = objects[0]\n",
    "            \n",
    "            # Get the neighbors within t\n",
    "            neighboring_sets = get_set_number_of_neighbors_within_t_ordered_by_t(np.array(objectt.vector['embedding']), t, X_train, y_train_set_labels, 'cosine')\n",
    "            assert len(neighboring_sets.shape) == 1, \"Expected neighboring_sets to be a 1D array\"\n",
    "\n",
    "            if is_in_sample:\n",
    "                assert len(neighboring_sets) >= 1, \"Expected at least the test image itself, because it is in the DB\"\n",
    "                assert neighboring_sets[0] == set_num, \"Expected the best distance set to be the test image itself\"\n",
    "                neighboring_sets = neighboring_sets[1:]\n",
    "\n",
    "            if is_in_sample:\n",
    "                if set_num in neighboring_sets:\n",
    "                    confusion_matrix.tp += 1\n",
    "                else:\n",
    "                    confusion_matrix.fp += 1\n",
    "            else:\n",
    "                if len(neighboring_sets) == 0:\n",
    "                    confusion_matrix.tn += 1\n",
    "                else:\n",
    "                    confusion_matrix.fn += 1\n",
    "\n",
    "cm = ConfusionMatrix()\n",
    "t = 0.1\n",
    "\n",
    "threshold_test(in_sample_series, X_train, y_train_set_labels, t, cm, is_in_sample=True)\n",
    "threshold_test(heldout_series, X_train, y_train_set_labels, t, cm, is_in_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee9a416c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfusionMatrix(tp=4788, tn=5527, fp=4508, fn=3773)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58449f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rescueCLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
